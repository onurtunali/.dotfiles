{"contents":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"biological-florida\",\n   \"metadata\": {},\n   \"source\": [\n    \"# 8 Feature Selection\\n\",\n    \"\\n\",\n    \"- Feature selection is about eliminating uninformative features so that computational complexity of training and inference decrease. Two main ideas play a role in the process:\\n\",\n    \"    - **Variance limit**: Eliminating inputs with low variance\\n\",\n    \"    - **Target relationship**: Measuring the relationship of a feature with the target variable through a chosen criteria such as correlation, mutual information etc. and then eliminating the ones with values lower than a threshold.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 49,\n   \"id\": \"framed-sharp\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%matplotlib inline\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import numpy as np\\n\",\n    \"import sklearn\\n\",\n    \"from sklearn import datasets\\n\",\n    \"import pandas as pd\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"fantastic-edinburgh\",\n   \"metadata\": {},\n   \"source\": [\n    \"- Let's start with variance limit. Using the Iris data, we can transform dataset features with variances lower than 0.2.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"id\": \"exposed-cardiff\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"sepal_length    0.685694\\n\",\n      \"sepal_width     0.189979\\n\",\n      \"petal_length    3.116278\\n\",\n      \"petal_width     0.581006\\n\",\n      \"target          0.671141\\n\",\n      \"dtype: float64\\n\"\n     ]\n    },\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"sepal_length    0.685694\\n\",\n       \"petal_length    3.116278\\n\",\n       \"petal_width     0.581006\\n\",\n       \"target          0.671141\\n\",\n       \"dtype: float64\"\n      ]\n     },\n     \"execution_count\": 2,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"iris_data = datasets.load_iris()\\n\",\n    \"df = pd.DataFrame(data=np.c_[iris_data.data, iris_data.target], \\n\",\n    \"                  columns=list(iris_data.feature_names) + ['target'])\\n\",\n    \"\\n\",\n    \"df.columns = ['sepal_length', 'sepal_width', \\n\",\n    \"              'petal_length', 'petal_width', \\n\",\n    \"              'target']\\n\",\n    \"print(df.var())\\n\",\n    \"var_limit = 0.2\\n\",\n    \"\\n\",\n    \"drops = [item for item in df.var().index if df.var()[item] < 0.2]\\n\",\n    \"df.drop(drops, inplace=True, axis=1)\\n\",\n    \"df.var()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"selective-supply\",\n   \"metadata\": {},\n   \"source\": [\n    \"- Or we can `sklearn` library, however results is not a `DataFrame` rather it's a numpy array.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"id\": \"dense-queens\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"array([[5.1, 1.4, 0.2, 0. ],\\n\",\n       \"       [4.9, 1.4, 0.2, 0. ],\\n\",\n       \"       [4.7, 1.3, 0.2, 0. ],\\n\",\n       \"       [4.6, 1.5, 0.2, 0. ],\\n\",\n       \"       [5. , 1.4, 0.2, 0. ],\\n\",\n       \"       [5.4, 1.7, 0.4, 0. ],\\n\",\n       \"       [4.6, 1.4, 0.3, 0. ],\\n\",\n       \"       [5. , 1.5, 0.2, 0. ],\\n\",\n       \"       [4.4, 1.4, 0.2, 0. ],\\n\",\n       \"       [4.9, 1.5, 0.1, 0. ]])\"\n      ]\n     },\n     \"execution_count\": 3,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"from sklearn.feature_selection import VarianceThreshold\\n\",\n    \"df = pd.DataFrame(data=np.c_[iris_data.data, iris_data.target], \\n\",\n    \"                  columns=list(iris_data.feature_names) + ['target'])\\n\",\n    \"\\n\",\n    \"df.columns = ['sepal_length', 'sepal_width', \\n\",\n    \"              'petal_length', 'petal_width', \\n\",\n    \"              'target']\\n\",\n    \"\\n\",\n    \"var_limit = VarianceThreshold(threshold=0.2)\\n\",\n    \"data_transformed = var_limit.fit_transform(df)\\n\",\n    \"data_transformed[:10]\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"5337ddf8-7be2-4707-97f9-a771e19e7bb3\",\n   \"metadata\": {},\n   \"source\": [\n    \"- A key point is that scaling features change the mean and variance. For example given a random variable $X \\\\sim D$ and $E[X]=\\\\mu, Var[X] = \\\\sigma^{2}$, scaling would generate a new random variable $Z=\\\\frac{x - \\\\mu}{\\\\sigma}$ with (using linearity of expected value and variance formulas) $E[Z] = 0, Var[Z] = 1$.\\n\",\n    \"\\n\",\n    \"- Another approach removing features with high correlation. Following example shows that `sepal_length` and `sl_square` are highly correlated, therefore a candidate for removal. \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"id\": \"random-finland\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>sepal_length</th>\\n\",\n       \"      <th>sepal_width</th>\\n\",\n       \"      <th>petal_length</th>\\n\",\n       \"      <th>petal_width</th>\\n\",\n       \"      <th>target</th>\\n\",\n       \"      <th>sl_squar</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>sepal_length</th>\\n\",\n       \"      <td>1.000000</td>\\n\",\n       \"      <td>-0.117570</td>\\n\",\n       \"      <td>0.871754</td>\\n\",\n       \"      <td>0.817941</td>\\n\",\n       \"      <td>0.782561</td>\\n\",\n       \"      <td>0.996842</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>sepal_width</th>\\n\",\n       \"      <td>-0.117570</td>\\n\",\n       \"      <td>1.000000</td>\\n\",\n       \"      <td>-0.428440</td>\\n\",\n       \"      <td>-0.366126</td>\\n\",\n       \"      <td>-0.426658</td>\\n\",\n       \"      <td>-0.104895</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>petal_length</th>\\n\",\n       \"      <td>0.871754</td>\\n\",\n       \"      <td>-0.428440</td>\\n\",\n       \"      <td>1.000000</td>\\n\",\n       \"      <td>0.962865</td>\\n\",\n       \"      <td>0.949035</td>\\n\",\n       \"      <td>0.858514</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>petal_width</th>\\n\",\n       \"      <td>0.817941</td>\\n\",\n       \"      <td>-0.366126</td>\\n\",\n       \"      <td>0.962865</td>\\n\",\n       \"      <td>1.000000</td>\\n\",\n       \"      <td>0.956547</td>\\n\",\n       \"      <td>0.801247</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>target</th>\\n\",\n       \"      <td>0.782561</td>\\n\",\n       \"      <td>-0.426658</td>\\n\",\n       \"      <td>0.949035</td>\\n\",\n       \"      <td>0.956547</td>\\n\",\n       \"      <td>1.000000</td>\\n\",\n       \"      <td>0.768566</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>sl_squar</th>\\n\",\n       \"      <td>0.996842</td>\\n\",\n       \"      <td>-0.104895</td>\\n\",\n       \"      <td>0.858514</td>\\n\",\n       \"      <td>0.801247</td>\\n\",\n       \"      <td>0.768566</td>\\n\",\n       \"      <td>1.000000</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"              sepal_length  sepal_width  petal_length  petal_width    target  \\\\\\n\",\n       \"sepal_length      1.000000    -0.117570      0.871754     0.817941  0.782561   \\n\",\n       \"sepal_width      -0.117570     1.000000     -0.428440    -0.366126 -0.426658   \\n\",\n       \"petal_length      0.871754    -0.428440      1.000000     0.962865  0.949035   \\n\",\n       \"petal_width       0.817941    -0.366126      0.962865     1.000000  0.956547   \\n\",\n       \"target            0.782561    -0.426658      0.949035     0.956547  1.000000   \\n\",\n       \"sl_squar          0.996842    -0.104895      0.858514     0.801247  0.768566   \\n\",\n       \"\\n\",\n       \"              sl_squar  \\n\",\n       \"sepal_length  0.996842  \\n\",\n       \"sepal_width  -0.104895  \\n\",\n       \"petal_length  0.858514  \\n\",\n       \"petal_width   0.801247  \\n\",\n       \"target        0.768566  \\n\",\n       \"sl_squar      1.000000  \"\n      ]\n     },\n     \"execution_count\": 4,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"df['sl_squar'] = df.sepal_length.apply(np.square)\\n\",\n    \"df.corr()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"f973ebe6-7f25-411a-8adb-dcbc7b39ab1c\",\n   \"metadata\": {},\n   \"source\": [\n    \"- Now, we can move on to target relationship which is named as *univariate feature selection*. In short, we choose a feature and measure its relationship with the target variable using a certain criteria. Note that $\\\\chi^{2}$ is only for non-negative data. \\n\",\n    \"\\n\",\n    \"- Following `class` chooses either k best features or percentage of feature ranked according to given criteria.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"id\": \"actual-accountability\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from sklearn.feature_selection import chi2\\n\",\n    \"from sklearn.feature_selection import f_classif\\n\",\n    \"from sklearn.feature_selection import f_regression\\n\",\n    \"from sklearn.feature_selection import mutual_info_classif\\n\",\n    \"from sklearn.feature_selection import mutual_info_regression\\n\",\n    \"from sklearn.feature_selection import SelectKBest\\n\",\n    \"from sklearn.feature_selection import SelectPercentile\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"class UnivariateFeatureSelection:\\n\",\n    \"    def __init__(self, n_features, problem_type, scoring):\\n\",\n    \"        \\n\",\n    \"        if problem_type == \\\"classification\\\":\\n\",\n    \"            valid_scoring = {\\n\",\n    \"                'f_classif': f_classif,\\n\",\n    \"                'chi2' : chi2,\\n\",\n    \"                'mutual_info_classif': mutual_info_classif\\n\",\n    \"            }\\n\",\n    \"        else:\\n\",\n    \"            valid_scoring = {\\n\",\n    \"                'f_regression': f_regression,\\n\",\n    \"                'mutual_info_regression': mutual_info_regression\\n\",\n    \"            }\\n\",\n    \"            \\n\",\n    \"        if scoring not in valid_scoring:\\n\",\n    \"            raise Exception('Invalid Scoring Function')\\n\",\n    \"        \\n\",\n    \"        if isinstance(n_features, int):\\n\",\n    \"            self.selection = SelectKBest(\\n\",\n    \"            valid_scoring[scoring],\\n\",\n    \"            k=n_features)\\n\",\n    \"            \\n\",\n    \"        elif isinstance(n_features, float):\\n\",\n    \"            self.selection = SelectPercentile(\\n\",\n    \"            valid_scoring[scoring],\\n\",\n    \"            percentile=int(n_features*100)\\n\",\n    \"            )\\n\",\n    \"        else:\\n\",\n    \"            raise Exception('Invalid type of features')\\n\",\n    \"            \\n\",\n    \"    def fit(self, X, y):\\n\",\n    \"        return self.selection.fit(X, y)\\n\",\n    \"    \\n\",\n    \"    def transform(self, X):\\n\",\n    \"        return self.selection.transform(X)\\n\",\n    \"    \\n\",\n    \"    def fit_transform(self, X, y):\\n\",\n    \"        return self.selection.fit_transform(X, y)\\n\",\n    \"                \"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"99cdf038-0158-499a-a073-ff06eee51947\",\n   \"metadata\": {},\n   \"source\": [\n    \"- Using `iris` dataset, we can show how selection works.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"id\": \"controlled-circumstances\",\n   \"metadata\": {\n    \"tags\": []\n   },\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"array([[5.1, 1.4, 0.2],\\n\",\n       \"       [4.9, 1.4, 0.2],\\n\",\n       \"       [4.7, 1.3, 0.2],\\n\",\n       \"       [4.6, 1.5, 0.2],\\n\",\n       \"       [5. , 1.4, 0.2],\\n\",\n       \"       [5.4, 1.7, 0.4],\\n\",\n       \"       [4.6, 1.4, 0.3],\\n\",\n       \"       [5. , 1.5, 0.2],\\n\",\n       \"       [4.4, 1.4, 0.2],\\n\",\n       \"       [4.9, 1.5, 0.1]])\"\n      ]\n     },\n     \"execution_count\": 7,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"X, y = datasets.load_iris(return_X_y=True)\\n\",\n    \"\\n\",\n    \"ufs = UnivariateFeatureSelection(n_features=3, \\n\",\n    \"                                 problem_type='regression', scoring='f_regression')\\n\",\n    \"\\n\",\n    \"ufs.fit(X,y)\\n\",\n    \"X_transformed = ufs.transform(X)\\n\",\n    \"X_transformed[:10]\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"6164164d-b01d-45ce-b726-33bdddcbf1e1\",\n   \"metadata\": {},\n   \"source\": [\n    \"- Greedy feature selection first evaluates the score of a single feature, then adds another to check whether is the score improved. If no improvement is achieved, it stops and returns the selected features. Since model performance is measured over and over again with different combinations of features, it's rather costly.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 36,\n   \"id\": \"powered-treasure\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Feature size (1000, 10)\\n\",\n      \"Feature size transformed (1000, 7)\\n\",\n      \"Scores [0.9218507496119938, 0.9276388422214754, 0.9280748491975872, 0.9282468519496312, 0.9283868541896669, 0.9283988543816701, 0.9284068545096722]\\n\",\n      \"Iterations[10, 20, 30, 40, 50, 60, 70]\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"from sklearn import linear_model\\n\",\n    \"from sklearn import metrics\\n\",\n    \"from sklearn.datasets import make_classification, make_regression\\n\",\n    \"iterations = [0]\\n\",\n    \"\\n\",\n    \"class GreedyFeatureSelection:\\n\",\n    \"    \\n\",\n    \"    def evaluate_score(self, X, y):\\n\",\n    \"        \\n\",\n    \"        model = linear_model.LogisticRegression()\\n\",\n    \"        model.fit(X, y)\\n\",\n    \"        predictions = model.predict_proba(X)[:,1]\\n\",\n    \"        auc = metrics.roc_auc_score(y, predictions)\\n\",\n    \"        return auc\\n\",\n    \"    \\n\",\n    \"    def _feature_selection(self, X, y):\\n\",\n    \"        global iterations\\n\",\n    \"        good_features = []\\n\",\n    \"        best_scores = []\\n\",\n    \"        \\n\",\n    \"        num_features = X.shape[1]\\n\",\n    \"        \\n\",\n    \"        while True:\\n\",\n    \"            count = 0\\n\",\n    \"            \\n\",\n    \"            this_feature = None\\n\",\n    \"            best_score = 0\\n\",\n    \"            \\n\",\n    \"            for feature in range(num_features):\\n\",\n    \"                count += 1\\n\",\n    \"                if feature in good_features:\\n\",\n    \"                    continue\\n\",\n    \"                selected_features = good_features + [feature]\\n\",\n    \"                xtrain = X[:, selected_features]\\n\",\n    \"                score = self.evaluate_score(xtrain, y)\\n\",\n    \"                \\n\",\n    \"                if score > best_score:\\n\",\n    \"                    this_feature = feature\\n\",\n    \"                    best_score = score\\n\",\n    \"                    \\n\",\n    \"            if this_feature != None:\\n\",\n    \"                good_features.append(this_feature)\\n\",\n    \"                best_scores.append(best_score)\\n\",\n    \"            iterations.append(iterations[-1]+count)\\n\",\n    \"            if len(best_scores) > 2:\\n\",\n    \"                if best_scores[-1] < best_scores[-2]:\\n\",\n    \"                    break\\n\",\n    \"                    \\n\",\n    \"        return best_scores[:-1], good_features[:-1]\\n\",\n    \"    \\n\",\n    \"    def __call__(self, X, y):\\n\",\n    \"        scores, features = self._feature_selection(X, y)\\n\",\n    \"        return X[:,features], scores\\n\",\n    \"    \\n\",\n    \"X, y = make_classification(n_samples=1000, n_features=10)\\n\",\n    \"print(f'Feature size {X.shape}')\\n\",\n    \"X_transformed, scores = GreedyFeatureSelection()(X, y)\\n\",\n    \"print(f'Feature size transformed {X_transformed.shape}')\\n\",\n    \"\\n\",\n    \"print(f'Scores {scores}\\\\nIterations{iterations[1:-1]}')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"5129c71b-b3c2-4027-9286-d84ddcf90dff\",\n   \"metadata\": {},\n   \"source\": [\n    \"- Recursive feature elimination\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 46,\n   \"id\": \"ee61b002-2d45-4254-8f27-86746b0fce57\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import pandas as pd\\n\",\n    \"\\n\",\n    \"from sklearn.feature_selection import RFE\\n\",\n    \"from sklearn.linear_model import LinearRegression\\n\",\n    \"from sklearn.datasets import fetch_california_housing\\n\",\n    \"\\n\",\n    \"data = fetch_california_housing()\\n\",\n    \"X = data['data']\\n\",\n    \"col_names = data['feature_names']\\n\",\n    \"y = data['target']\\n\",\n    \"\\n\",\n    \"model = LinearRegression()\\n\",\n    \"\\n\",\n    \"rfe = RFE(estimator=model, n_features_to_select=3)\\n\",\n    \"\\n\",\n    \"X_transformed = rfe.fit_transform(X, y)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"9af28e6c-f5ab-473e-a503-36e6ac05a8e7\",\n   \"metadata\": {},\n   \"source\": [\n    \"- Feature importance \"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 52,\n   \"id\": \"3b3587af-b057-491d-9ff9-d5b250baf594\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"image/png\": \"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\\n\",\n      \"text/plain\": [\n       \"<Figure size 432x288 with 1 Axes>\"\n      ]\n     },\n     \"metadata\": {\n      \"needs_background\": \"light\"\n     },\n     \"output_type\": \"display_data\"\n    }\n   ],\n   \"source\": [\n    \"from sklearn.datasets import load_diabetes\\n\",\n    \"from sklearn.ensemble import RandomForestRegressor\\n\",\n    \"\\n\",\n    \"data = load_diabetes()\\n\",\n    \"X = data['data']\\n\",\n    \"col_names = data['feature_names']\\n\",\n    \"y = data['target']\\n\",\n    \"\\n\",\n    \"model = RandomForestRegressor()\\n\",\n    \"model.fit(X,y)\\n\",\n    \"\\n\",\n    \"importances = model.feature_importances_\\n\",\n    \"idxs = np.argsort(importances)\\n\",\n    \"\\n\",\n    \"plt.barh(range(len(idxs)), importances[idxs], align='center')\\n\",\n    \"plt.yticks(range(len(idxs)), [col_names[i] for i in idxs])\\n\",\n    \"plt.xlabel('Random forest feature importances')\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"28e08240-2c5b-4c58-bb32-5a7c48fa5ec3\",\n   \"metadata\": {},\n   \"source\": [\n    \"- Also we can use or interpret another model's parameters as feature importance metric.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 53,\n   \"id\": \"512f1d1c-da2f-4d0a-b45c-266b4006d798\",\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"['bmi', 's5']\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"from sklearn.datasets import load_diabetes\\n\",\n    \"from sklearn.feature_selection import SelectFromModel\\n\",\n    \"from sklearn.ensemble import RandomForestRegressor\\n\",\n    \"\\n\",\n    \"data = load_diabetes()\\n\",\n    \"X = data['data']\\n\",\n    \"y = data['target']\\n\",\n    \"col_names = data['feature_names']\\n\",\n    \"\\n\",\n    \"model = RandomForestRegressor()\\n\",\n    \"sfm = SelectFromModel(estimator=model)\\n\",\n    \"X_transform = sfm.fit_transform(X, y)\\n\",\n    \"\\n\",\n    \"support = sfm.get_support()\\n\",\n    \"\\n\",\n    \"print([x for x, y in zip(col_names, support) if y == True])\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.7.6\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}","lastModifiedTimeMs":1622828556495}